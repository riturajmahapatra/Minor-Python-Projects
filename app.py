import streamlit as st
import requests
import nltk
from nltk.tokenize import sent_tokenize

# Download tokenizer (runs once)
nltk.download("punkt")

# Streamlit Page Config
st.set_page_config(page_title="AI Flashcards", page_icon="ğŸ“–", layout="centered")

# Sidebar Header
st.sidebar.title("ğŸ“Œ Instructions")
st.sidebar.write("1ï¸âƒ£ Enter a topic in the box.")  
st.sidebar.write("2ï¸âƒ£ Click 'Generate Flashcards'.")  
st.sidebar.write("3ï¸âƒ£ Click on a card to reveal the answer.")  

# App Title & Description
st.title("ğŸ“š AI-Powered Flashcards")
st.markdown("**Generate learning flashcards from Wikipedia summaries.**")

# User Input
topic = st.text_input("ğŸ” Enter a topic:").strip().replace(" ", "_")

if st.button("âš¡ Generate Flashcards"):
    if topic:
        URL = f"https://en.wikipedia.org/api/rest_v1/page/summary/{topic}"
        response = requests.get(URL)

        if response.status_code == 200:
            data = response.json()
            sentences = sent_tokenize(data.get("extract", ""))  

            if sentences:
                st.success(f"âœ… Flashcards for: **{topic.replace('_', ' ')}**")
                
                for i, sentence in enumerate(sentences[:5]):  
                    with st.expander(f"ğŸ’¡ Flashcard {i+1} (Click to Reveal)"):
                        st.write(sentence)
            else:
                st.warning("âš ï¸ No relevant data found. Try another topic.")
        else:
            st.error("âŒ Failed to fetch data. Please try again.")
    else:
        st.warning("âš ï¸ Please enter a valid topic.")